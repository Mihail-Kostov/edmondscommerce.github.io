<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: wget | Edmonds Commerce Dev Blog]]></title>
  <link href="http://edmondscommerce.github.io/tag/wget/atom.xml" rel="self"/>
  <link href="http://edmondscommerce.github.io/"/>
  <updated>2015-06-18T19:29:38+01:00</updated>
  <id>http://edmondscommerce.github.io/</id>
  <author>
    <name><![CDATA[EdmondsCommerce Development Team]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Wget For Beginners]]></title>
    <link href="http://edmondscommerce.github.io/linux/wget-for-beginners.html"/>
    <updated>2012-11-19T08:02:25+00:00</updated>
    <id>http://edmondscommerce.github.io/linux/wget-for-beginners</id>
    <content type="html"><![CDATA[<h3>What is Wget in general ?</h3>


<p>Wget is a free utility for non-interactive download of file from the web. The user doesnâ€™t need to login system every time <a href="http://www.gnu.org/software/wget/" rel="nofollow">Wget</a> can download the entire web page or mirroring the entire web page. If download crashed or stop for various reason Wget will start download again from where it stopped. It is highly recommended for downloading file from web with slow network connections.</p>

<h2>How to Use Wget ? </h2>


<p>```php</p>

<p>wget -t 10 www.google.com</p>

<p>```</p>

<p>If network connection fails Wget will to try to reconnect 20 times in default.With -t command we can specify how many times it need to  reconnect.</p>

<p>```php</p>

<p>wget -p &mdash;convert-links -r www.google.com -o logfile</p>

<p>```</p>

<p>This command will download the site -p and &mdash;convert will make sure all linked files are linked to downloaded document such as images and external links it enables complete offline viewing. Log file can be enabled with -O command to view the output message.</p>

<p>```php</p>

<p>wget &mdash;spider &mdash;force-html www.google.com</p>

<p>```</p>

<p>&mdash;spider Will check the webpage is existent or not.
&mdash;force  It will enforce the file type that have to be downloaded.</p>

<p>```php</p>

<p>wget -u mozilla www.google.com</p>

<p>```</p>

<p>some site access allowed to certain user <a href="http://www.user-agents.org/" rel="nofollow">agents</a>. So to access with certain user agent you can use this command.</p>

<p>Ftp connection download is achieved by</p>

<p>```php</p>

<p>wget -r <a href="ftp://username:password@ftp.example.com">ftp://username:password@ftp.example.com</a></p>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cron Creating Lots of Files in Home Directory]]></title>
    <link href="http://edmondscommerce.github.io/linux/cron-creating-lots-of-files-in-home-directory.html"/>
    <updated>2009-09-22T15:55:14+01:00</updated>
    <id>http://edmondscommerce.github.io/linux/cron-creating-lots-of-files-in-home-directory</id>
    <content type="html"><![CDATA[<div class="oldpost"><h4>This is post is now quite old and the the information it contains may be out of date or innacurate.</h4>
<p>
If you find any errors or have any suggestions to update the information <a href="http://edmondscommerce.github.io/contact-us/index.html">please let us know</a>
or <a href="https://github.com/edmondscommerce/edmondscommerce.github.io">create a pull request on GitHub</a>
</p>
</div>


<p>If you have some cron jobs set up and you are finding large amounts of files saved in your home directory (or root) then perhaps you have the same issue I had.</p>

<p>I was using wget to call on some PHP scripts to run periodically. wget will do what it says on the tin and save the files. If you don&rsquo;t want it to do that, you need to add the following in front of your wget command:
```</p>

<p>-O /dev/null</p>

<p>```</p>

<p>eg
```</p>

<p>wget -O /dev/null <a href="http://script.com/script.php">http://script.com/script.php</a></p>

<p>```</p>
]]></content>
  </entry>
  
</feed>

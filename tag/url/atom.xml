<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: url | Edmonds Commerce Dev Blog]]></title>
  <link href="http://edmondscommerce.github.io/tag/url/atom.xml" rel="self"/>
  <link href="http://edmondscommerce.github.io/"/>
  <updated>2015-02-12T15:36:15+00:00</updated>
  <id>http://edmondscommerce.github.io/</id>
  <author>
    <name><![CDATA[EdmondsCommerce Development Team]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Getting sub domains and domain from a url]]></title>
    <link href="http://edmondscommerce.github.io/php/getting-sub-domains-and-domain-from-a-url.html"/>
    <updated>2014-11-14T13:34:08+00:00</updated>
    <id>http://edmondscommerce.github.io/php/getting-sub-domains-and-domain-from-a-url</id>
    <content type="html"><![CDATA[<p>At first it seems like a simple one &ndash; just use the <code>parse_url</code> function already built into PHP.</p>

<p>Unfortunately though, this just gives you the host &ndash; which is usually a combination of a subdomain &ndash; such as www &ndash; and the domain itself.</p>

<p>To resolve this I ended up writing the following code. It will gradually snip bits off the start of a host and then make a HTTP request until that request fails.</p>

<p>Once it fails, we know that we have snipped off all of the sub domains we can and we have our final domain.</p>

<p>Here is the code:</p>

<p>``` php</p>

<pre><code>public function getDomain($url)
{
    if (!$this-&gt;_domain) {
        $host = parse_url($url, PHP_URL_HOST);
        $sanity = $this-&gt;getHeadersForUrl($host);
        if (!$sanity) {
            throw new ErrorException("No headers when sanity checking full host: $host in " . __METHOD__);
        }
        $elems = explode('.', $host);
        $subdomains = array();
        $domain = false;
        while (!$domain) {
            $check = implode('.', $elems);
            $headers = $this-&gt;getHeadersForUrl($check);
            if ($headers) {
                $subdomains[] = array_shift($elems);
            } else {
                array_unshift($elems, array_pop($subdomains));
                $domain = implode('.', $elems);
            }
        }
        $this-&gt;_domain = $domain;
    }
    return $this-&gt;_domain;
}

public function getHeadersForUrl($url, $followOnLocation = true)
{
    $ch = curl_init();
    curl_setopt($ch, CURLOPT_URL, $url);
    curl_setopt($ch, CURLOPT_HEADER, true);
    curl_setopt($ch, CURLOPT_NOBODY, true);
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
    curl_setopt($ch, CURLOPT_FOLLOWLOCATION, $followOnLocation);
    curl_setopt($ch, CURLOPT_MAXREDIRS, 10);
    curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, false);
    curl_setopt($ch, CURLOPT_SSL_VERIFYHOST, false);

    $data = curl_exec($ch);

    return $data;
}
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Mod Rewrite and Escaped Hashes (and other characters)]]></title>
    <link href="http://edmondscommerce.github.io/apache/apache-mod-rewrite-and-escaped-hashes-and-other-characters.html"/>
    <updated>2013-04-22T13:57:49+01:00</updated>
    <id>http://edmondscommerce.github.io/apache/apache-mod-rewrite-and-escaped-hashes-and-other-characters</id>
    <content type="html"><![CDATA[<p>If you are having issues with mod rewrite apparently abandoning sections of variables after encoded hashes then this could be your solution.</p>

<p>The use case is particularly clear when using mod_rewrite in front of a search engine (such as Solr which I am really enjoying working with at the moment).</p>

<p>Imagine someone search for a partcode ABC#123</p>

<p>This gets encoded to search/abc%23123</p>

<p>Your rewritten search term will then be mangled by Apache and your search script will only actually see ABC. That is of course a problem and the solution is not really clear.</p>

<p>After a bit of searching around I found the solution is to add a B flag to your mod_rewrite rule so that mod_rewrite will escape these characters so that they are cleanly passed through.</p>

<p>For example:</p>

<p>```php</p>

<p>RewriteRule ^(.<em>)search/(.</em>)$ advanced_search_result.php?keywords=$2 [L,B]</p>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Mod Rewrite Escaped Slashes Problem + Solution]]></title>
    <link href="http://edmondscommerce.github.io/apache/apache-mod-rewrite-escaped-slashes-problem-solution.html"/>
    <updated>2013-04-11T11:05:20+01:00</updated>
    <id>http://edmondscommerce.github.io/apache/apache-mod-rewrite-escaped-slashes-problem-solution</id>
    <content type="html"><![CDATA[<p>As part of our Magento SEO service, the first thing we do is to make sure there are no issues with the crawlability and general health of the clients web site.</p>

<p>Whilst working on the Google Webmaster Tools crawl errors for a client I noticed one specific and intruiging problem for which I couldn&rsquo;t immediately see the reason, everything looked to be set up perfectly.</p>

<p>Certain URLs were getting 404 responses. The URL was being parsed by mod_rewrite but everything looked fine so why was apache giving a 404?</p>

<p>The problem turns out to be that the URLs contain escaped slashes (eg search/KTA-mb667k2%2F2g),</p>

<p>The problem is that Apache actually handles the escaped slash and helpfully converts it to a real slash. That then means that it is trying to look in a sub folder that does not exist and hence the 404.</p>

<p>Chances are you don&rsquo;t want escaped slashes to really be thought of as real directory separating slashes, especially if you are using mod_rewrite.</p>

<p>The answer is a simple one liner to be added to your vhost.conf or httpd.conf.</p>

<p>```</p>

<p>AllowEncodedSlashes On</p>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Disable the Default DuckDuckGo Seach Engine in Firefox on Linux Mint?]]></title>
    <link href="http://edmondscommerce.github.io/linux%20mint/how-to-disabled-the-default-duckduckgo-seach-engine-in-firefox-on-linux-mint.html"/>
    <updated>2013-01-07T15:12:14+00:00</updated>
    <id>http://edmondscommerce.github.io/linux%20mint/how-to-disabled-the-default-duckduckgo-seach-engine-in-firefox-on-linux-mint</id>
    <content type="html"><![CDATA[<p>If you are using Firefox browser on Linux Mint you might have notice that the default search engine is not Google but DuckDuckGo.</p>

<p>DuckDuckGo are presenting themselves as the biggest threat to Google search engine bringing as the main advantage more privacy than what Google currently offer.</p>

<p>If you are happy with it than you are fine but if you feel like me that DuckDuckGo might have some interesting features, it does not bring the same result or way of using internet than Google search engine then you might want to come back to Google then please follow the below step by step procedure:</p>

<pre><code>Type about:config in the location bar and press Enter

Type keyword.URL in filter

Change the keyword.URL value to:

http://www.google.co.uk/search?ie=UTF-8&amp;oe=UTF-8&amp;sourceid=navclient&amp;gfns=1&amp;q=
</code></pre>

<p>Hope that help.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Remove Double Slashes from URL without Breaking It]]></title>
    <link href="http://edmondscommerce.github.io/php/remove-double-slashes-from-url-without-breaking-it.html"/>
    <updated>2012-02-24T16:58:38+00:00</updated>
    <id>http://edmondscommerce.github.io/php/remove-double-slashes-from-url-without-breaking-it</id>
    <content type="html"><![CDATA[<p>If you need to clean up a URL and remove any double (or more) slashes that might have crept in, but need to keep the :// bit intact you might like this little function</p>

<p>```php</p>

<pre><code>protected function removeDoubleSlash($in) {
    return preg_replace('%([^:])(
</code></pre>

<p>```</p>

<p>Handy :)</p>
]]></content>
  </entry>
  
</feed>

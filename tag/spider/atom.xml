<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: spider | Edmonds Commerce Dev Blog]]></title>
  <link href="http://edmondscommerce.github.io/tag/spider/atom.xml" rel="self"/>
  <link href="http://edmondscommerce.github.io/"/>
  <updated>2015-06-17T12:40:40+01:00</updated>
  <id>http://edmondscommerce.github.io/</id>
  <author>
    <name><![CDATA[EdmondsCommerce Development Team]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Apache Access Logs Find Spiders by Counting Requests to IP Addresses]]></title>
    <link href="http://edmondscommerce.github.io/linux/apache-access-logs-find-spiders-by-counting-requests-to-ip-addresses.html"/>
    <updated>2012-08-21T13:24:37+01:00</updated>
    <id>http://edmondscommerce.github.io/linux/apache-access-logs-find-spiders-by-counting-requests-to-ip-addresses</id>
    <content type="html"><![CDATA[<p>If you would like a quick summary of the IP addresses that are beating the **** out of your server by firing lots of requests for quite possibly malicious or otherwise nefarious reasons then try this little bash script:</p>

<p>```bash</p>

<h1>!/bin/bash</h1>

<p>LOG_FILE=/var/www/vhosts/DOMAIN.co.uk/statistics/logs/access_log
OUT_FILE=/tmp/spider_analysis</p>

<h1>This generates a file with the top 20 IP addresses by number of requests</h1>

<p>cat $LOG_FILE | awk &lsquo;{print $1}&rsquo; | sort | uniq -c | sort -nr | head -n 20 > $OUT_FILE</p>

<p>echo &ldquo;Top 20 IP addresses by number of request&rdquo;
cat $OUT_FILE</p>

<h1>allow for loop to split on new line</h1>

<p>IFS_BAK=$IFS
IFS=&ldquo;
&rdquo;</p>

<p>for i in <code>cat $OUT_FILE</code>
do</p>

<pre><code>COUNT=`echo $i | awk '{print $1}'`
IP_ADD=`echo $i | awk '{print $2}'`
echo ""
echo "---------------------------------"
echo ""
echo "$IP_ADD has made $COUNT requests"
echo "Whois Information"
whois $IP_ADD 
#lynx -dump http://who.cc/$IP_ADD # whois was blocked on the server i was using for some reason, use lynx as a work around
echo ""
echo "---------------------------------"
echo ""
</code></pre>

<p>done</p>

<h1>set that back</h1>

<p>IFS=$IFS_BAK
IFS_BAK=</p>

<p>```</p>

<p>You would use this to give you some idea of which IPs are hitting the server a lot.</p>

<p>Usually you would expect to see a lot of these being search engines which you likely want to allow. However if you see any domestic or other IP addresses then you may choose to block these.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building Spiders: Grab Data, Post Forms and Interact with Web Sites Automatically]]></title>
    <link href="http://edmondscommerce.github.io/php/curl/spidering/firefox/programming/building-spiders-grab-data-post-forms-and-interact-with-web-sites-automatically.html"/>
    <updated>2008-02-14T11:56:16+00:00</updated>
    <id>http://edmondscommerce.github.io/php/curl/spidering/firefox/programming/building-spiders-grab-data-post-forms-and-interact-with-web-sites-automatically</id>
    <content type="html"><![CDATA[<div class="oldpost"><h4>This is post is now quite old and the the information it contains may be out of date or innacurate.</h4>
<p>
If you find any errors or have any suggestions to update the information <a href="http://edmondscommerce.github.io/contact-us/index.html">please let us know</a>
or <a href="https://github.com/edmondscommerce/edmondscommerce.github.io">create a pull request on GitHub</a>
</p>
</div>


<p>One of the most useful and powerful things you can do with PHP is to create a programme which will simulate a web browser and can grab data, post data to forms and generally interact with other web sites &ndash; automatically.</p>

<p>For PHP to be able to work like this it must have the CURL library installed and active. It is the CURL library which actually handles all of the interaction and PHP is my scripting language of choice for interacting with CURL.</p>

<p>A simple CURL function is like this:</p>

<p>```php</p>

<p>function curl($url){</p>

<p>$timeout = &lsquo;300&rsquo;; //how long before CURL gives up on this page
$go = curl_init();
curl_setopt ($go, CURLOPT_URL, $url);
curl_setopt ($go, CURLOPT_RETURNTRANSFER, 1);
curl_setopt ($go, CURLOPT_FOLLOWLOCATION, 1);
curl_setopt ($go, CURLOPT_TIMEOUT, $timeout);
$page = curl_exec($go);
curl_close($go);
return $page;</p>

<p>}</p>

<p>```</p>

<p>This function when called and echoed will output the entire html of the $url specified.</p>

<p>For grabbing data from this page to be inserted into a database (for example when spidering a suppliers web site for product information to be inserted into your site) we then use regular expressions to find what we are looking for and then insert that into the database.</p>

<p>so for example if we wanted to grab the product title and we knew that this was wrapped in a h1 tag with the class &ldquo;product title&rdquo; we could use this regexp to grab this:</p>

<p>```php</p>

<p>$page = curl($url);</p>

<p>$pattern = &lsquo;%</p>

<h1 class="product_title">(.+?)</h1>


<p>%i';</p>

<p>preg_match($pattern,$page,$matches);</p>

<p>print_r($matches); //we can see the entire array of matches and choose which we want to insert into the database</p>

<p>```</p>

<p>We can also Post data to web sites using curl. This allows us to do all kinds of things including grabbing data that is displayed on the submission of post forms. Here is an example Curl Post Function:
```php</p>

<p>function curl_post($url,$post_data){</p>

<p>$timeout = &lsquo;300&rsquo;; //how long before CURL gives up on this page
$go = curl_init();
curl_setopt ($go, CURLOPT_URL, $url);
curl_setopt ($go, CURLOPT_RETURNTRANSFER, 1);
curl_setopt ($go, CURLOPT_FOLLOWLOCATION, 1);
curl_setopt ($go, CURLOPT_TIMEOUT, $timeout);
//now for the post section
curl_setopt($go, CURLOPT_POST, true);</p>

<p>curl_setopt($go, CURLOPT_POSTFIELDS, $post_data);
$page = curl_exec($go);
curl_close($go);
return $page;
}</p>

<p>```</p>

<p>It can be tricky to figure out exactly what data should be in the post string. To help you out though is this incredibly handy addon for firefox: <a href="https://addons.mozilla.org/en-US/firefox/addon/3829" target="_blank">Live Http Headers</a>.</p>

<p>This addon lets you see exactly what is going on between your browser and the web site you are visiting. This can quickly and easily give you the information you need to replicate the same behaviour with your CURL script.</p>

<p>Edmonds Commerce specialise in working with PHP and CURL. If you have any spidering, screen scraping or other application that requires PHP to actively interact with other web sites &ndash; <a href="http://www.edmondscommerce.co.uk/contact-about-curl-and-php.html">get in touch today</a> to see how we can help you benefit from this incredibly powerful technique.</p>

<p>Related Resources</p>

<p><a href="http://www.phpfour.com/blog/2008/01/20/php-http-class/" rel="nofollow"><a href="http://www.phpfour.com/blog/2008/01/20/php-http-class/">http://www.phpfour.com/blog/2008/01/20/php-http-class/</a></a></p>

<p><a href="http://www.phpclasses.org/browse/package/1988.html" rel="nofollow"><a href="http://www.phpclasses.org/browse/package/1988.html">http://www.phpclasses.org/browse/package/1988.html</a></a></p>

<p><a href="http://www.phpit.net/article/using-curl-php/" rel="nofollow"><a href="http://www.phpit.net/article/using-curl-php/">http://www.phpit.net/article/using-curl-php/</a></a></p>

<p><a href="http://skeymedia.com/intro-to-curl-with-php/" rel="nofollow"><a href="http://skeymedia.com/intro-to-curl-with-php/">http://skeymedia.com/intro-to-curl-with-php/</a></a></p>
]]></content>
  </entry>
  
</feed>

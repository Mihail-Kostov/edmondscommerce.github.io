<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: sitemap | Edmonds Commerce Dev Blog]]></title>
  <link href="http://edmondscommerce.github.io/tag/sitemap/atom.xml" rel="self"/>
  <link href="http://edmondscommerce.github.io/"/>
  <updated>2015-06-24T10:23:39+01:00</updated>
  <id>http://edmondscommerce.github.io/</id>
  <author>
    <name><![CDATA[EdmondsCommerce Development Team]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Crawl an xml sitemap quality check 301 and 404]]></title>
    <link href="http://edmondscommerce.github.io/php/crawl-an-xml-sitemap-quality-check-301-and-404.html"/>
    <updated>2012-01-03T17:54:30+00:00</updated>
    <id>http://edmondscommerce.github.io/php/crawl-an-xml-sitemap-quality-check-301-and-404</id>
    <content type="html"><![CDATA[<p>On occasion, google and other tools will tell you there&rsquo;s errors with your sitemap.xml file and not give you the information of what exactly is wrong, so we wrote this little tool to crawl the sitemap and check for 301 redirections and 404 errors.</p>

<p>It is a noddy file and should have much more error handling etc but here&rsquo;s the basic flow :&ndash;</p>

<p>```php</p>

<p>&lt;?php
$screenwidth = 80;
function trimstr($str, $maxlength = -1, $middle = &lsquo;&hellip;&rsquo;) {</p>

<pre><code>global $screenwidth;
if ($maxlength == -1) {
    $maxlength = $screenwidth - 1;
}
if (count($str) &gt; $maxlength) {
    $partlength = round($maxlength - count($middle) / 2);
    $leftpart = substr($str, 0, $partlength);
    $rightpart = substr($str, 0-$partlength);
    return $leftpart . $middle . $rightpart;
} else {
    return $str;
}
</code></pre>

<p>}</p>

<p>// First we load the sitemap xml
$xml = simplexml_load_file($argv[1]);
$counter = 0;
$threadcount = 5;
$multihandle = curl_multi_init();
$fourohfours = $threeohones = $twohundreds = 0;
if ($xml->getName() != &lsquo;urlset&rsquo;) {</p>

<pre><code>die("Doesn't look like a valid sitemap!");
</code></pre>

<p>}
$total_urls = count($xml->children());</p>

<p>// Then we iterate over it
foreach($xml->children() as $child)
{</p>

<pre><code>if ($child-&gt;getName() == 'url') {
    foreach ($child-&gt;children() as $subchild) {
        if ($subchild-&gt;getName() == 'loc') {
            echo "Fetching : ".trimstr($subchild, $screenwidth - 12)."\n";
            $counter++;
            addurltostack($subchild);
            if ($counter%$threadcount == 0 || $counter == $total_urls) {
                do {
                    curl_multi_exec($multihandle, $running);
                } while ($running &gt; 0);
                processresults();
                echo "\n".$counter.'/'.$total_urls.' urls checked - '.$twohundreds.' 200s; '.$threeohones.' 301s; '.$fourohfours.' 404s.'."\n";
            }
        }
    }
}
</code></pre>

<p>}
echo &ldquo;\n&rdquo;;
if ($fourohfours > 0) {</p>

<pre><code>echo "The following urls were 404 returns :- \n";
foreach ($fourohfoururls as $url) {
    echo $url."\n";
}
</code></pre>

<p>}
if ($threeohones > 0) {</p>

<pre><code>echo "The following urls were 301 returns :- \n";
foreach ($threeohoneurls as $url) {
    echo $url."\n";
}
</code></pre>

<p>}
function addurltostack($url) {</p>

<pre><code>global $curls;
global $multihandle;
$ch = curl_init();
$curls[] = $ch;
// Set the url path we want to call
curl_setopt($ch, CURLOPT_URL, $url);
// Make it so the data coming back is put into a string
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true); 
curl_setopt($ch, CURLOPT_HEADER, TRUE); 
curl_setopt($ch, CURLOPT_NOBODY, TRUE); // remove body 
curl_multi_add_handle($multihandle, $ch);
</code></pre>

<p>}</p>

<p>function processresults() {</p>

<pre><code>global $curls;
global $multihandle;
global $fourohfours;
global $fourohfoururls;
global $threeohones;
global $threeohoneurls;
global $twohundreds;
global $multihandle;
foreach ($curls as $ch) {
    $httpCode = curl_getinfo($ch, CURLINFO_HTTP_CODE); 
    $url = curl_getinfo($ch, CURLINFO_EFFECTIVE_URL); 
    // Free up the resources $ch is using
    curl_multi_remove_handle($multihandle,$ch);
    curl_close($ch);
    switch($httpCode) {
        case 400 : 
            $fourohfours++;
            $fourohfoururls[] = $url;
        break;
        case 301 : 
            $threeohones++;
            $threeohoneurls[] = $url;
        break;
        case 200 : $twohundreds++;
        break;
    }
}
curl_multi_close($multihandle);
$multihandle = curl_multi_init();
$curls = array();
</code></pre>

<p>}
?></p>

<p>```</p>

<p>The above takes about &frac34; hour for an approximately 3000 link sitemap.  This script utilises curl_multi so it runs 5 requests at a time to the server.</p>
]]></content>
  </entry>
  
</feed>

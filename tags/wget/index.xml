<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Wget on Edmonds Commerce Dev Blog</title>
    <link>https://edmondscommerce.github.io/tags/wget/</link>
    <description>Recent content in Wget on Edmonds Commerce Dev Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 19 Nov 2012 08:02:25 +0000</lastBuildDate>
    <atom:link href="https://edmondscommerce.github.io/tags/wget/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Wget For Beginners</title>
      <link>https://edmondscommerce.github.io/linux/wget-for-beginners.html</link>
      <pubDate>Mon, 19 Nov 2012 08:02:25 +0000</pubDate>
      
      <guid>https://edmondscommerce.github.io/linux/wget-for-beginners.html</guid>
      <description>&lt;h3&gt;What is Wget in general ?&lt;/h3&gt;

&lt;p&gt;Wget is a free utility for non-interactive download of file from the web. The user doesnâ€™t need to login system every time &lt;a href=&#34;http://www.gnu.org/software/wget/&#34; rel=&#34;nofollow&#34;&gt;Wget&lt;/a&gt; can download the entire web page or mirroring the entire web page. If download crashed or stop for various reason Wget will start download again from where it stopped. It is highly recommended for downloading file from web with slow network connections.&lt;/p&gt;

&lt;h2&gt;How to Use Wget ? &lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-php&#34; data-lang=&#34;php&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;x&#34;&gt;wget -t 10 www.google.com&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If network connection fails Wget will to try to reconnect 20 times in default.With -t command we can specify how many times it need to  reconnect.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-php&#34; data-lang=&#34;php&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;x&#34;&gt;wget -p --convert-links -r www.google.com -o logfile&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This command will download the site -p and &amp;ndash;convert will make sure all linked files are linked to downloaded document such as images and external links it enables complete offline viewing. Log file can be enabled with -O command to view the output message.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-php&#34; data-lang=&#34;php&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;x&#34;&gt;wget --spider --force-html www.google.com&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&amp;ndash;spider Will check the webpage is existent or not.
&amp;ndash;force  It will enforce the file type that have to be downloaded.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-php&#34; data-lang=&#34;php&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;x&#34;&gt;wget -u mozilla www.google.com&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;some site access allowed to certain user &lt;a href=&#34;http://www.user-agents.org/&#34; rel=&#34;nofollow&#34;&gt;agents&lt;/a&gt;. So to access with certain user agent you can use this command.&lt;/p&gt;

&lt;p&gt;Ftp connection download is achieved by&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-php&#34; data-lang=&#34;php&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;x&#34;&gt;wget -r ftp://username:password@ftp.example.com&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Cron Creating Lots of Files in Home Directory</title>
      <link>https://edmondscommerce.github.io/linux/cron-creating-lots-of-files-in-home-directory.html</link>
      <pubDate>Tue, 22 Sep 2009 15:55:14 +0000</pubDate>
      
      <guid>https://edmondscommerce.github.io/linux/cron-creating-lots-of-files-in-home-directory.html</guid>
      <description>&lt;p&gt;&lt;div class=&#34;oldpost&#34;&gt;&lt;h4&gt;This is post is now quite old and the the information it contains may be out of date or innacurate.&lt;/h4&gt;
&lt;p&gt;
If you find any errors or have any suggestions to update the information &lt;a href=&#34;http://edmondscommerce.github.io/contact-us/index.html&#34;&gt;please let us know&lt;/a&gt;
or &lt;a href=&#34;https://github.com/edmondscommerce/edmondscommerce.github.io&#34;&gt;create a pull request on GitHub&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
If you have some cron jobs set up and you are finding large amounts of files saved in your home directory (or root) then perhaps you have the same issue I had.&lt;/p&gt;

&lt;p&gt;I was using wget to call on some PHP scripts to run periodically. wget will do what it says on the tin and save the files. If you don&amp;rsquo;t want it to do that, you need to add the following in front of your wget command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
-O /dev/null 

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;eg&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
wget -O /dev/null http://script.com/script.php

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>